熵：服从某一特定概率分布事件的理论最小平均编码长度
关于交叉熵在loss函数中使用
	1.信息量： 事件发生的概率越低，信息量越大
	
	2.熵：所有信息量的期望
		
	3.相对熵（KL散度）: 如果用P来描述目标问题，而不是用q来描述，得到的信息增量
	
	理解：当p和q相等时，信息增量为0。
	4. 交叉熵

	相对熵变形，得到：
	
	其中的交叉熵如下：
	
	
	理解：相对熵（信息增量）等效为交叉熵减去金标准的信息熵。简单理解时，可以直接把交叉熵理解为相对熵，因为金标准的信息熵不变。
	5. 关于机器学习中常用的交叉熵计算：
	
	应用场景：每一个输出都只有两种可能值，即为二项分布；
	解释：y为金标准的第一类事件发生对应的概率；y'为预测该第一类事件发生对应的概率；1-y为金标准对应的另一类事件发生对应的概率；（1-y')为预测该另一类事件发生对应概率
	这种方式设计损失函数的好处：log的存在，使得偏离标准值越远，惩罚越大；
交叉熵优点：1）非负；2）能根据当前值偏离理想值的程度，给予不同的惩罚

GAN的前世今生
	
						
						GAN的训练函数
常规的GAN难以训练的原因：
1）第一类GAN：
原理：
等价在最优判别器下最小化生成分布与真实分布之间的JS散度；
表现：
判别器效果越好，生成器梯度消失越严重；
---原因：随机生成分布很难与真实分布有不可忽略的重叠以及JS散度的突变特性
2）第二类GAN：
原理：
等价于最小化生成分布与真实分布直接的KL散度，又要最大化其JS散度；
表现：
损失函数相互矛盾，导致梯度不稳定，而且KL散度的不对称性是的生成器宁可丧失多样性也不愿意丧失准确性；
生成器宁可生成重复但安全的样本，也不愿意生成多样性的样本。即collapse mode；
3）WGAN前作：
针对分布重叠问题提出了，通过对生成样本和真实样本加噪声使得两个分布产生重叠，解决了训练不稳定的问题；
其缺陷在于：未能提供一个指示训练进程的可靠指标，也未做实验验证。
4）WGAN本作引入了Wesserstein距离
原理：Wesserstein距离相对散度具有优越的平衡性；通过数学将该距离写成可求解的形式；
特点：从分类问题变成回归问题，因此不需要sigmoid；

1）生成器随机初始化后的生成分布很难与真实分布有不可忽略的重叠；
2）等价优化的距离衡量（KL散度，JS散度）不合理；






Cycle-GAN原理：







Learning Invariant Representation for Unsupervised Image Restoration

核心思想：学习一个噪声和干净图像的共有分布；
（区别于cycle-gan：学习一个域到另一个域的变换）
思路是：抽象出两者都包含的中间分布，以及噪声分布，通过这两个分布，我们可以构建任意的噪声图，和干净图；
1）中间域z描述：

z即为交集的分布；
2）网络中的前向约束如下：
	
	其中第一个公式：先从噪声图中生成中间分布，再从该中间分布生成干净图；
	第二个公式：通过干净的图像生成中间分布，再将该分布加上噪声的分布，然后通过神经网络处理，得到干净的分布；（这里基于的思想是直接从
3）网络中的后向约束如下：
	
	1.前向生成干净图后，现在通过求其中间分布图，加上对应的噪声图，再经过生成器，得到对应的噪声图；（这就是个cycle-gan的思路）
	2.前向生成的噪声图后，求其中间分布图，再通过生成网络，得到干净图；
4）所以总体的cycle-gan的loss如下：

5）需要对z域进行约束，保证无论是x，还是y，生成的是同一个z域分布；

这里利用了判别函数进行约束；
6）由于上述都是非监督学习，可以会有域的漂移，即细节不匹配；
所以引入了自监督模块：通过高斯卷积，保证模型输出的结果，和带噪图，经过高斯模糊后的，结构的一致性；还有perceptual结构的loss；



7）域对抗loss，就是为了去验证生成的最终图像的真假；（这是和传统的gan，以及cycle-gan的思路是一样的，判断生成器生成的图像的真假）

8）self-Reconstruction loss:自己生成自己
9）KL Loss：
	用于约束噪声的分布接近正态分布；
	
总体loss：

其中：第一个loss用于判断生成正确的中间态，其最佳值为2log(0.5)，
第二个loss用于判断最终最终转化成的x，和y是否为真，其最佳值也为2log(0.5)
第三个loss用于判断交叉域的结果生成的准确性，即从x通过encodex变为中间态z，再将z先变为y，再从y经过一系列变换，变为x，求x的差别；
第四个loss用于判断单域下的结果生成的准确性，即从x通过encodex变为中间态z，再将z直接变为x，求x的差别；
第五个loss是将x和生成的x经过高斯滤波后的结果对比；
第六个loss是将x和生成的x经过vgg网络得到的语义信息的对比；
第七个loss是为了约束噪声分布；

关于神经网络的泛化性研究---极市平台公众号
论文：High-frequency Component Helps Explain the Generalization of Convolutional
Neural Networks
文章主要内容：
	1）将图像的高频信息和低频信息分开成两幅图像，神经网络可以单从部分高频信息中检测到正确的结果，而包含更多信息的低频却出现误差；
	2）针对现象一的分析，文章认为：
		在一个有限的数据集中，除了数据本身的内容和label的关联外，还有些label和其中的高频噪音的关联。因此模型可能会无差别地学习数据本身的信号或者这些高频信号；
		即模型学习到了高频和语义的混合信息；
	3）BN的优势在于，其提升了高频信息的量值，导致可以学到更多的高频信息；
